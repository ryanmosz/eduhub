Description: Testing efficiency rule - AUTOMATE BEFORE ASKING for manual tests
Globs: **/*.py, **/*.md, **/test*.py, **/tasks*.md

# Testing Efficiency & Automation Rule

## üö® CRITICAL: Terminal Command Visibility

**PROVEN WORKING METHOD** - User must see real-time progress, not spinning circles:

### ‚ùå WRONG - Hidden Loops (User Sees Spinning Circle)
```bash
# USER SEES: Spinning circle for 6+ seconds ‚Üí thinks it's hung ‚Üí interrupts
for i in {1..12}; do curl "http://localhost:8000/test"; sleep 0.5; done
```

### ‚úÖ CORRECT - Chained Commands (User Sees Each Step)
```bash
# USER SEES: Each result appears immediately as it executes
echo "Testing rate limiting - 12 requests..." && curl -w "Request 1: %{http_code}\n" "http://localhost:8000/auth/login" && curl -w "Request 2: %{http_code}\n" "http://localhost:8000/auth/login" && curl -w "Request 3: %{http_code}\n" "http://localhost:8000/auth/login" && echo "Rate limiting test complete"
```

### **VERIFIED RULES (TESTED & WORKING):**
1. **`is_background: false`** - User sees output in real-time
2. **Chain with `&&`** - Each command shows immediately when it finishes
3. **No loops** - Use individual chained commands instead
4. **Progress indicators** - `echo` statements and `-w` flags show what's happening
5. **No silent gaps >3 seconds** - If any step takes longer, break it down further

### **EMERGENCY RULE**
If user sees spinning circle >3 seconds = They WILL interrupt thinking it's hung.
**Solution**: Chain individual commands so each step appears immediately.

### **Examples - TESTED WORKING**
```bash
# ‚úÖ WORKING: 12-step rate limit test - each result appears immediately
echo "Testing..." && curl -w "Request 1: %{http_code}\n" "url1" && curl -w "Request 2: %{http_code}\n" "url2" && echo "Complete"

# ‚úÖ WORKING: Progress test - user sees each step
echo "Starting..." && sleep 1 && echo "Step 1 complete" && sleep 1 && echo "Step 2 complete" && echo "Done"

# ‚ùå BROKEN: User sees spinning circle, thinks it's hung
for i in {1..12}; do curl "http://localhost:8000/test"; done
```

---

## CRITICAL PRINCIPLE: **Automate Before Asking**

**MANDATORY**: Before requesting ANY manual testing from the user, MUST re-evaluate if the test can be automated programmatically. Manual testing is a major efficiency blocker and should only be used as a last resort.

## Testing Hierarchy (STRICT ORDER)

### 1. **Programmatic Unit Tests** ‚≠ê (ALWAYS PREFER)
```python
# Example: Direct function testing
python test_integration.py
from src.module.function import test_function
result = test_function(test_data)
assert result == expected
```

### 2. **API Testing with curl/httpx** ‚≠ê (ALWAYS PREFER)
```bash
# Test server health
curl http://localhost:8000/

# Test endpoints programmatically
curl -H "Authorization: Bearer $JWT" http://localhost:8000/auth/user
```

### 3. **Integration Test Scripts** ‚≠ê (ALWAYS PREFER)
- Write async Python scripts for complex flows
- Mock external services when testing integration
- Test error handling and edge cases programmatically

### 4. **Manual Testing** ‚ùå (LAST RESORT ONLY)
- **MUST JUSTIFY**: Explain why automation isn't possible
- **PROCESS**:
  1. Re-evaluate first - can this be automated?
  2. If user says "reevaluate" ‚Üí find automated alternative
  3. Only proceed if automation truly impossible

## Decision Tree - FOLLOW THIS EXACTLY

```
Testing request ‚Üí Is it manual? ‚Üí NO ‚Üí Proceed with automated test
                                ‚Üì YES
Can this be tested with curl/HTTP? ‚Üí YES ‚Üí Use curl/httpx instead
                                   ‚Üì NO
Can logic be unit tested? ‚Üí YES ‚Üí Write Python test script
                          ‚Üì NO
Can external services be mocked? ‚Üí YES ‚Üí Write integration test with mocks
                                 ‚Üì NO
Is this one-time verification? ‚Üí YES ‚Üí Consider browser automation
                              ‚Üì NO
                              ‚Üí MUST automate or reconsider approach
```

## IMPLEMENTATION RULES

### DO ‚úÖ (Required Actions):
- Write test scripts for any complex logic
- Use curl/httpx for API endpoint testing
- Mock external services (Auth0, Plone, etc.) when testing
- Test graceful fallbacks programmatically
- Create reusable test utilities
- Test error handling with scripts

### DON'T ‚ùå (Forbidden Actions):
- Ask for manual testing without re-evaluation
- Require browser interaction for API tests
- Test one-off scenarios manually when scriptable
- Skip automated testing because "it works"
- Assume manual testing is faster

## Available Testing Tools

### Command Line Testing:
```bash
# Server operations
pkill -f uvicorn
source .venv/bin/activate.fish && uvicorn src.eduhub.main:app --reload --host 127.0.0.1 --port 8000

# Health checks
curl http://localhost:8000/
curl -I http://localhost:8000/docs

# API testing
curl -X POST -H "Content-Type: application/json" -d '{"key":"value"}' http://localhost:8000/endpoint
```

### Python Testing:
```python
# Direct function testing
from src.module import function
result = function(test_input)

# Async testing
import asyncio
result = asyncio.run(async_function())

# HTTP client testing
import httpx
response = httpx.get("http://localhost:8000/endpoint")
assert response.status_code == 200
```

## Examples of Correct Application

### ‚úÖ GOOD: Auth Integration Testing
```python
# Create test script: test_auth_integration.py
def test_role_extraction():
    from src.eduhub.auth.plone_bridge import extract_roles_from_auth0
    result = extract_roles_from_auth0({"email": "admin@example.com"})
    assert "Manager" in result

# Run: python test_auth_integration.py
```

### ‚ùå BAD: Manual Testing Request
"Go to the browser, login, click buttons, check if user info shows roles"

### ‚úÖ GOOD: API Endpoint Testing
```bash
# Test endpoint availability
curl http://localhost:8000/auth/user
# Expected: 403 (no token) or 200 (with token)
```

### ‚ùå BAD: Manual API Testing
"Open Swagger UI, click the endpoint, enter data, check response"

## Efficiency Benefits

- **Automated tests**: Seconds to run, repeatable, catch regressions
- **Manual tests**: Minutes per test, error-prone, need re-testing after changes
- **ROI**: 1 hour automation = 10+ hours manual testing saved
- **Velocity**: Continuous development without bottlenecks

## Enforcement

When user says "reevaluate":
1. **IMMEDIATELY** reconsider the testing approach
2. Find programmatic alternative
3. Implement automated test instead
4. Only request manual testing if automation is genuinely impossible

**Goal**: Enable continuous development velocity through automated verification. Every avoided manual test increases development speed and software quality.

**Remember**: When tempted to ask for manual testing, first ask: "How can I test this programmatically?"
